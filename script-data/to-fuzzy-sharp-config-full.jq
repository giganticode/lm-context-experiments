.bidir=.arch.bidir | .model=(.arch.name | ascii_upcase) | .emsize=.arch.emb_sz | .nhid=.arch.n_hid | .nlayers=.arch.n_layers | .lr=.training.schedule.init_lr | .clip=.training.gradient_clip | .batch_size=.bs | del(.bs) | .tied=.arch.tie_weights | .dropouth=.arch.drop.multiplier*.arch.drop.outh | .dropouti=.arch.drop.multiplier*.arch.drop.outi | .dropoute=.arch.drop.multiplier*.arch.drop.oute | .wdrop=.arch.drop.multiplier*.arch.drop.w | .dropout=.arch.drop.multiplier*.arch.drop.out | .bidir=.arch.bidir | .out_bias=.arch.out_bias | del(.arch) | .alpha=.training.activation_regularization.alpha | .beta=.training.activation_regularization.beta | .epochs=.training.schedule.max_epochs | .extension=.corpus.extensions | del(.corpus) | .prep_function_callable=.prep_function.callable | .prep_function_param=.prep_function.params[0] | .no_com=.prep_function.options.no_com | .no_str=.prep_function.options.no_str | .no_spaces=.prep_function.options.no_spaces | .no_unicode=.prep_function.options.no_unicode | del(.prep_function) | .optimizer=.training.optimizer.name | .momentum=.training.optimizer.momentum | .max_lr_reduction_times=.schedule.max_lr_reduction_times | .mult_coeff=.chedule.mult_coeff | .schedule=.training.schedule.name | .patience=.training.schedule.patience | .sub_epochs=.training.sub_epochs | .wdecay=.training.weight_decay | del(.training)
